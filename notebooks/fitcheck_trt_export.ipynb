{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b419b98-f0c0-4d22-9c18-fa055727d4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/razaare/anaconda3/envs/fitcheck/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnxruntime\n",
    "import transformers, numpy as np\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "from huggingface_hub import snapshot_download\n",
    "from diffusers import AutoencoderKL, DDIMScheduler, UNet2DConditionModel\n",
    "from torchinfo import summary\n",
    "from accelerate import load_checkpoint_in_model\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc959249-2d53-4d0c-ac71-d2859c7ec241",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipAttnProcessor(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        attn,\n",
    "        hidden_states,\n",
    "        encoder_hidden_states=None,\n",
    "        attention_mask=None,\n",
    "        temb=None,\n",
    "    ):\n",
    "        return hidden_states\n",
    "\n",
    "class AttnProcessor2_0(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "    Processor for implementing scaled dot-product attention (enabled by default if you're using PyTorch 2.0).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size=None,\n",
    "        cross_attention_dim=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if not hasattr(F, \"scaled_dot_product_attention\"):\n",
    "            raise ImportError(\"AttnProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.\")\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        attn,\n",
    "        hidden_states,\n",
    "        encoder_hidden_states=None,\n",
    "        attention_mask=None,\n",
    "        temb=None,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        residual = hidden_states\n",
    "\n",
    "        if attn.spatial_norm is not None:\n",
    "            hidden_states = attn.spatial_norm(hidden_states, temb)\n",
    "\n",
    "        input_ndim = hidden_states.ndim\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            batch_size, channel, height, width = hidden_states.shape\n",
    "            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)\n",
    "\n",
    "        batch_size, sequence_length, _ = (\n",
    "            hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape\n",
    "        )\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)\n",
    "            # scaled_dot_product_attention expects attention_mask shape to be\n",
    "            # (batch, heads, source_length, target_length)\n",
    "            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])\n",
    "\n",
    "        if attn.group_norm is not None:\n",
    "            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "        query = attn.to_q(hidden_states)\n",
    "\n",
    "        if encoder_hidden_states is None:\n",
    "            encoder_hidden_states = hidden_states\n",
    "        elif attn.norm_cross:\n",
    "            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)\n",
    "\n",
    "        key = attn.to_k(encoder_hidden_states)\n",
    "        value = attn.to_v(encoder_hidden_states)\n",
    "\n",
    "        inner_dim = key.shape[-1]\n",
    "        head_dim = inner_dim // attn.heads\n",
    "\n",
    "        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "\n",
    "        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)\n",
    "\n",
    "        # the output of sdp = (batch, num_heads, seq_len, head_dim)\n",
    "        # TODO: add support for attn.scale when we move to Torch 2.1\n",
    "        \n",
    "        hidden_states = F.scaled_dot_product_attention(\n",
    "            query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False\n",
    "        )\n",
    "        # hidden_states = flash_attn_func(\n",
    "        #     query, key, value, dropout_p=0.0, causal=False\n",
    "        # )\n",
    "\n",
    "        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)\n",
    "        hidden_states = hidden_states.to(query.dtype)\n",
    "\n",
    "        # linear proj\n",
    "        hidden_states = attn.to_out[0](hidden_states)\n",
    "        # dropout\n",
    "        hidden_states = attn.to_out[1](hidden_states)\n",
    "\n",
    "        if input_ndim == 4:\n",
    "            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)\n",
    "\n",
    "        if attn.residual_connection:\n",
    "            hidden_states = hidden_states + residual\n",
    "\n",
    "        hidden_states = hidden_states / attn.rescale_output_factor\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3de1cbea-c727-4530-a0f2-ed50da0b4572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_adapter(unet, \n",
    "                 cross_attn_cls=SkipAttnProcessor,\n",
    "                 self_attn_cls=None,\n",
    "                 cross_attn_dim=None, \n",
    "                 **kwargs):\n",
    "    if cross_attn_dim is None:\n",
    "        cross_attn_dim = unet.config.cross_attention_dim\n",
    "    attn_procs = {}\n",
    "    for name in unet.attn_processors.keys():\n",
    "        cross_attention_dim = None if name.endswith(\"attn1.processor\") else cross_attn_dim\n",
    "        if name.startswith(\"mid_block\"):\n",
    "            hidden_size = unet.config.block_out_channels[-1]\n",
    "        elif name.startswith(\"up_blocks\"):\n",
    "            block_id = int(name[len(\"up_blocks.\")])\n",
    "            hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n",
    "        elif name.startswith(\"down_blocks\"):\n",
    "            block_id = int(name[len(\"down_blocks.\")])\n",
    "            hidden_size = unet.config.block_out_channels[block_id]\n",
    "        if cross_attention_dim is None:\n",
    "            if self_attn_cls is not None:\n",
    "                attn_procs[name] = self_attn_cls(hidden_size=hidden_size, cross_attention_dim=cross_attention_dim, **kwargs)\n",
    "            else:\n",
    "                # retain the original attn processor\n",
    "                attn_procs[name] = AttnProcessor2_0(hidden_size=hidden_size, cross_attention_dim=cross_attention_dim, **kwargs)\n",
    "        else:\n",
    "            attn_procs[name] = cross_attn_cls(hidden_size=hidden_size, cross_attention_dim=cross_attention_dim, **kwargs)\n",
    "                                                    \n",
    "    unet.set_attn_processor(attn_procs)\n",
    "    adapter_modules = torch.nn.ModuleList(unet.attn_processors.values())\n",
    "    return adapter_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a315798-0865-4a2e-a7ed-ef17f493ddb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trainable_module(unet, trainable_module_name):\n",
    "    if trainable_module_name == \"unet\":\n",
    "        return unet\n",
    "    elif trainable_module_name == \"transformer\":\n",
    "        trainable_modules = torch.nn.ModuleList()\n",
    "        for blocks in [unet.down_blocks, unet.mid_block, unet.up_blocks]:\n",
    "            if hasattr(blocks, \"attentions\"):\n",
    "                trainable_modules.append(blocks.attentions)\n",
    "            else:\n",
    "                for block in blocks:\n",
    "                    if hasattr(block, \"attentions\"):\n",
    "                        trainable_modules.append(block.attentions)\n",
    "        return trainable_modules\n",
    "    elif trainable_module_name == \"attention\":\n",
    "        attn_blocks = torch.nn.ModuleList()\n",
    "        for name, param in unet.named_modules():\n",
    "            if \"attn1\" in name:\n",
    "                attn_blocks.append(param)\n",
    "        return attn_blocks\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown trainable_module_name: {trainable_module_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd75fa5a-ebb1-4092-9f6e-975ef7f6bead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_attn_ckpt_load(attn_ckpt, version):\n",
    "    sub_folder = {\n",
    "        \"mix\": \"mix-48k-1024\",\n",
    "        \"vitonhd\": \"vitonhd-16k-512\",\n",
    "        \"dresscode\": \"dresscode-16k-512\",\n",
    "    }[version]\n",
    "    if os.path.exists(attn_ckpt):\n",
    "        load_checkpoint_in_model(attn_modules, os.path.join(attn_ckpt, sub_folder, 'attention'))\n",
    "    else:\n",
    "        repo_path = snapshot_download(repo_id=attn_ckpt)\n",
    "        print(f\"Downloaded {attn_ckpt} to {repo_path}\")\n",
    "        load_checkpoint_in_model(attn_modules, os.path.join(repo_path, sub_folder, 'attention'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39cfb7bc-ac47-4eb1-bf3d-c5f24f4e569f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path = f\"booksforcharlie/stable-diffusion-inpainting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "deed1cbf-94ca-4a7f-b371-a937bc3324a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 12 files: 100%|█████████████████████████████████████████████████████████████████████████| 12/12 [00:00<00:00, 134937.39it/s]\n"
     ]
    }
   ],
   "source": [
    "repo_path = snapshot_download(\"zhengchong/CatVTON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a54d370-e2bc-4f5a-8e19-3aae03103a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model_path = f\"stabilityai/sd-vae-ft-mse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a27aae4-4f3d-47bb-bea1-d445113c7bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch booksforcharlie/stable-diffusion-inpainting: booksforcharlie/stable-diffusion-inpainting does not appear to have a file named diffusion_pytorch_model.safetensors.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "/home/razaare/anaconda3/envs/fitcheck/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "unet = UNet2DConditionModel.from_pretrained(base_model_path, subfolder=\"unet\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79e5cef1-ee1e-432c-bc2c-d51736966242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): AttnProcessor2_0()\n",
       "  (1): SkipAttnProcessor()\n",
       "  (2): AttnProcessor2_0()\n",
       "  (3): SkipAttnProcessor()\n",
       "  (4): AttnProcessor2_0()\n",
       "  (5): SkipAttnProcessor()\n",
       "  (6): AttnProcessor2_0()\n",
       "  (7): SkipAttnProcessor()\n",
       "  (8): AttnProcessor2_0()\n",
       "  (9): SkipAttnProcessor()\n",
       "  (10): AttnProcessor2_0()\n",
       "  (11): SkipAttnProcessor()\n",
       "  (12): AttnProcessor2_0()\n",
       "  (13): SkipAttnProcessor()\n",
       "  (14): AttnProcessor2_0()\n",
       "  (15): SkipAttnProcessor()\n",
       "  (16): AttnProcessor2_0()\n",
       "  (17): SkipAttnProcessor()\n",
       "  (18): AttnProcessor2_0()\n",
       "  (19): SkipAttnProcessor()\n",
       "  (20): AttnProcessor2_0()\n",
       "  (21): SkipAttnProcessor()\n",
       "  (22): AttnProcessor2_0()\n",
       "  (23): SkipAttnProcessor()\n",
       "  (24): AttnProcessor2_0()\n",
       "  (25): SkipAttnProcessor()\n",
       "  (26): AttnProcessor2_0()\n",
       "  (27): SkipAttnProcessor()\n",
       "  (28): AttnProcessor2_0()\n",
       "  (29): SkipAttnProcessor()\n",
       "  (30): AttnProcessor2_0()\n",
       "  (31): SkipAttnProcessor()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_adapter(unet, cross_attn_cls=SkipAttnProcessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8d292c6-4174-40e8-be2d-ecc57da02e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_modules = get_trainable_module(unet, \"attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ede6f1a-6f91-4dbb-b3d0-2692ca1ff438",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_attn_ckpt_load(repo_path, \"mix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73c80529-7b57-4052-9c2f-9ec512c78126",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'noise_scheduler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m summary(\u001b[43mnoise_scheduler\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'noise_scheduler' is not defined"
     ]
    }
   ],
   "source": [
    "summary(noise_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a2e69bc-25f7-490f-af06-3bd3cd19542e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7355908d-f3f1-49cf-87fe-d4a623d6f052",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_h, target_w = 768, 1024\n",
    "latent_h, latent_w = target_h // 8, target_w // 8\n",
    "timestep = torch.tensor([0, 0]).to(device)          # or an int, or a 1-D tensor\n",
    "B = 1\n",
    "text_emb = torch.randn(B, 77, 768).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5a82862-2d91-43bf-a2d9-342601835ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "latents  = torch.randn(1, 9, 256, 96).to(device)\n",
    "timestep = torch.tensor([0]).to(device)   \n",
    "text_emb = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "374e7224-394f-450c-b51e-9671b62a624f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d790357-bad5-4295-bb01-7ca5b67df08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet2DConditionOutput(sample=tensor([[[[-0.0194,  0.6650, -0.6582,  ...,  0.0899,  0.0390, -0.3538],\n",
      "          [-0.4746, -0.0834,  0.0643,  ...,  0.4412,  0.3240, -0.0194],\n",
      "          [ 0.8023, -0.2351, -0.1483,  ..., -0.5015, -0.7761,  0.2852],\n",
      "          ...,\n",
      "          [ 0.2856,  0.3402, -0.8140,  ...,  0.0404, -1.4999, -0.3887],\n",
      "          [-0.3153,  0.0259,  0.2955,  ..., -0.6490, -1.6492,  0.6452],\n",
      "          [-0.2383, -0.5000,  0.4734,  ..., -0.1951,  0.0021,  0.1873]],\n",
      "\n",
      "         [[-0.3322,  1.0846, -0.0256,  ...,  0.2241, -0.5634,  0.4134],\n",
      "          [ 0.5437,  0.0455,  0.0151,  ...,  0.4621,  0.3210, -0.3693],\n",
      "          [ 0.6319, -0.5658,  0.6428,  ..., -0.2190,  0.6855,  0.0678],\n",
      "          ...,\n",
      "          [ 0.3675, -0.3273,  0.1165,  ..., -0.0463,  0.2239, -0.8052],\n",
      "          [-0.6482, -0.1556,  0.4305,  ..., -0.4057,  0.3460,  0.6349],\n",
      "          [ 0.2523,  0.0934,  0.5658,  ...,  0.2503,  0.1444, -0.8433]],\n",
      "\n",
      "         [[ 0.2852,  0.7701, -0.6358,  ..., -1.3040, -0.4034, -0.2569],\n",
      "          [-0.4108,  0.2020,  0.7391,  ...,  0.8579,  0.4500,  0.2372],\n",
      "          [-0.1027, -0.3701, -0.1689,  ..., -0.4615,  0.6999,  0.1192],\n",
      "          ...,\n",
      "          [ 0.2246,  0.2141, -0.1393,  ...,  0.7481, -0.6785, -0.3475],\n",
      "          [-1.1322, -0.7201, -0.2122,  ..., -0.3480,  1.5476, -0.5103],\n",
      "          [ 0.0345, -0.3954, -0.9279,  ..., -1.1187,  1.2805,  0.4122]],\n",
      "\n",
      "         [[ 0.1349, -2.0546,  0.9229,  ...,  1.0263, -0.1351,  0.6076],\n",
      "          [ 0.7313,  0.1670,  0.3690,  ..., -1.5182, -0.1607, -0.4424],\n",
      "          [ 0.0716,  0.9959,  0.4129,  ...,  0.8046,  0.0450,  0.6142],\n",
      "          ...,\n",
      "          [-0.2869, -0.6747,  0.1703,  ..., -0.6618,  1.3230,  0.1973],\n",
      "          [ 1.3012,  0.6019, -0.2462,  ...,  0.5113,  0.2378, -0.2284],\n",
      "          [ 0.2721, -0.1084,  0.7084,  ...,  1.2055, -0.7604, -0.7002]]]],\n",
      "       device='cuda:0', grad_fn=<ConvolutionBackward0>))\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "unet.to(device)\n",
    "B, H, W = 1, 256, 96                # 64×64 latents ⇒ 512×512 output after VAE\n",
    "cross_dim = unet.config.cross_attention_dim  # 768 for SD-1.5 / 1280 for SD-XL\n",
    "\n",
    "# ----- 2.  Dummy inputs ----------------------------------------------------\n",
    "dummy_inputs = {\n",
    "    \"sample\": torch.randn(B, 9, H, W).to(\"cuda\"),            # latent tensor\n",
    "    \"timestep\": torch.tensor([0]).to(\"cuda\"),                # or an int 0\n",
    "    \"encoder_hidden_states\": None,\n",
    "}\n",
    "\n",
    "out = unet(\n",
    " **dummy_inputs\n",
    ")\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4403e360-fce8-47d2-8927-d5177865e67c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 256, 96])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c230d58-c49d-42a0-a81a-0fbbe9ee2e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_axes = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7591e60a-1b4f-488f-b225-615d1d1bb03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/razaare/anaconda3/envs/fitcheck/lib/python3.10/site-packages/diffusers/models/downsampling.py:136: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert hidden_states.shape[1] == self.channels\n",
      "/home/razaare/anaconda3/envs/fitcheck/lib/python3.10/site-packages/diffusers/models/downsampling.py:145: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert hidden_states.shape[1] == self.channels\n",
      "/home/razaare/anaconda3/envs/fitcheck/lib/python3.10/site-packages/diffusers/models/upsampling.py:146: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert hidden_states.shape[1] == self.channels\n",
      "/home/razaare/anaconda3/envs/fitcheck/lib/python3.10/site-packages/diffusers/models/upsampling.py:162: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if hidden_states.shape[0] >= 64:\n",
      "/home/razaare/anaconda3/envs/fitcheck/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_condition.py:1302: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if not return_dict:\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(\n",
    "    unet,\n",
    "    (latents, timestep, text_emb),\n",
    "    \"unet.onnx\",\n",
    "    opset_version=17,\n",
    "    input_names=[\"sample\", \"timestep\"],\n",
    "    output_names = [\"out_sample\"],\n",
    "    dynamic_axes={\n",
    "        \"sample\": {0: \"batch\", 2: \"height\", 3: \"width\"},\n",
    "        \"timestep\": {0: \"batch\"},\n",
    "        \"out_sample\": {0: \"batch\", 2: \"height\", 3: \"width\"},\n",
    "    },\n",
    "    do_constant_folding=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6484d869-7aea-4732-89cb-a857ce7bc411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unet.onnx\n"
     ]
    }
   ],
   "source": [
    "!ls unet*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bdc7b68-241c-44a8-924b-e17fd85ccbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load(\"unet.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c64bdc8-c0d2-467f-bf81-7dab9caa2532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(onnx.checker.check_model(\"unet.onnx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4da7ffae-f24f-43c3-9d2f-60c1c9d6dcd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-07-13 18:18:34.624263338 [W:onnxruntime:, transformer_memcpy.cc:83 ApplyImpl] 16 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n",
      "\u001b[0;93m2025-07-13 18:18:34.629083935 [W:onnxruntime:, session_state.cc:1280 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-07-13 18:18:34.629090229 [W:onnxruntime:, session_state.cc:1282 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "ort_session = onnxruntime.InferenceSession(\"unet.onnx\", providers=[\"CUDAExecutionProvider\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55ffb673-06af-44f0-842f-1feb77bda3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a53d910a-c316-44ed-aecb-8f34badbf23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_inputs = {\n",
    "    \"sample\": to_numpy(latents),\n",
    "    \"timestep\": to_numpy(timestep)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43d115d2-e442-47a7-862c-7450a2c19665",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;31m2025-07-13 18:19:30.296926007 [E:onnxruntime:, sequential_executor.cc:572 ExecuteKernel] Non-zero status code returned while running Softmax node. Name:'/down_blocks.0/attentions.0/transformer_blocks.0/attn1/Softmax' Status Message: /onnxruntime_src/onnxruntime/core/framework/bfc_arena.cc:376 void* onnxruntime::BFCArena::AllocateRawInternal(size_t, bool, onnxruntime::Stream*, bool, onnxruntime::WaitNotificationFn) Failed to allocate memory for requested buffer of size 19327352832\n",
      "\u001b[m\n"
     ]
    },
    {
     "ename": "RuntimeException",
     "evalue": "[ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Non-zero status code returned while running Softmax node. Name:'/down_blocks.0/attentions.0/transformer_blocks.0/attn1/Softmax' Status Message: /onnxruntime_src/onnxruntime/core/framework/bfc_arena.cc:376 void* onnxruntime::BFCArena::AllocateRawInternal(size_t, bool, onnxruntime::Stream*, bool, onnxruntime::WaitNotificationFn) Failed to allocate memory for requested buffer of size 19327352832\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ort_outs \u001b[38;5;241m=\u001b[39m \u001b[43mort_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mort_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/fitcheck/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:273\u001b[0m, in \u001b[0;36mSession.run\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    271\u001b[0m     output_names \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_meta]\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m C\u001b[38;5;241m.\u001b[39mEPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "\u001b[0;31mRuntimeException\u001b[0m: [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Non-zero status code returned while running Softmax node. Name:'/down_blocks.0/attentions.0/transformer_blocks.0/attn1/Softmax' Status Message: /onnxruntime_src/onnxruntime/core/framework/bfc_arena.cc:376 void* onnxruntime::BFCArena::AllocateRawInternal(size_t, bool, onnxruntime::Stream*, bool, onnxruntime::WaitNotificationFn) Failed to allocate memory for requested buffer of size 19327352832\n"
     ]
    }
   ],
   "source": [
    "ort_outs = ort_session.run(None, ort_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83390bd3-ee9f-4447-ad57-09c4153f47a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0897c2aa-b06c-4554-8b6b-300f894d2126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: onnxruntime 1.22.1\n",
      "Uninstalling onnxruntime-1.22.1:\n",
      "  Successfully uninstalled onnxruntime-1.22.1\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall onnxruntime -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "743d61ce-40cb-47bc-8f32-0e28419600ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting onnxruntime-gpu\n",
      "  Downloading onnxruntime_gpu-1.22.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: coloredlogs in ./anaconda3/envs/fitcheck/lib/python3.10/site-packages (from onnxruntime-gpu) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in ./anaconda3/envs/fitcheck/lib/python3.10/site-packages (from onnxruntime-gpu) (25.2.10)\n",
      "Requirement already satisfied: numpy>=1.21.6 in ./anaconda3/envs/fitcheck/lib/python3.10/site-packages (from onnxruntime-gpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in ./anaconda3/envs/fitcheck/lib/python3.10/site-packages (from onnxruntime-gpu) (25.0)\n",
      "Requirement already satisfied: protobuf in ./anaconda3/envs/fitcheck/lib/python3.10/site-packages (from onnxruntime-gpu) (6.31.1)\n",
      "Requirement already satisfied: sympy in ./anaconda3/envs/fitcheck/lib/python3.10/site-packages (from onnxruntime-gpu) (1.14.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./anaconda3/envs/fitcheck/lib/python3.10/site-packages (from coloredlogs->onnxruntime-gpu) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./anaconda3/envs/fitcheck/lib/python3.10/site-packages (from sympy->onnxruntime-gpu) (1.3.0)\n",
      "Downloading onnxruntime_gpu-1.22.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (283.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.2/283.2 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: onnxruntime-gpu\n",
      "Successfully installed onnxruntime-gpu-1.22.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade onnxruntime-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c260f993-bc79-40ef-b356-5205abd30277",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "544fbcf8-8563-4060-b47f-3057b406de61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8747db9-c3ee-4c29-a87d-da678224cc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: preprocess.py [-h] --input INPUT --output OUTPUT\n",
      "                     [--skip_optimization SKIP_OPTIMIZATION]\n",
      "                     [--skip_onnx_shape SKIP_ONNX_SHAPE]\n",
      "                     [--skip_symbolic_shape SKIP_SYMBOLIC_SHAPE]\n",
      "                     [--auto_merge] [--int_max INT_MAX] [--guess_output_rank]\n",
      "                     [--verbose VERBOSE] [--save_as_external_data]\n",
      "                     [--all_tensors_to_one_file]\n",
      "                     [--external_data_location EXTERNAL_DATA_LOCATION]\n",
      "                     [--external_data_size_threshold EXTERNAL_DATA_SIZE_THRESHOLD]\n",
      "\n",
      "Model optimizer and shape inferencer, in preparation for quantization,\n",
      "Consists of three optional steps: 1. Symbolic shape inference (best for\n",
      "transformer models). 2. Model optimization. 3. ONNX shape inference. Model\n",
      "quantization with QDQ format, i.e. inserting QuantizeLinear/DeQuantizeLinear\n",
      "on the tensor, requires tensor shape information to perform its best.\n",
      "Currently, shape inferencing works best with optimized model. As a result, it\n",
      "is highly recommended to run quantization on optimized model with shape\n",
      "information. This is the tool for optimization and shape inferencing.\n",
      "Essentially this tool performs the following three (skippable) steps: 1.\n",
      "Symbolic shape inference. 2. Model optimization 3. ONNX shape inference\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --input INPUT         Path to the input model file\n",
      "  --output OUTPUT       Path to the output model file\n",
      "  --skip_optimization SKIP_OPTIMIZATION\n",
      "                        Skip model optimization step if true. It's a known\n",
      "                        issue that ORT optimization has difficulty with model\n",
      "                        size greater than 2GB, rerun with this option to get\n",
      "                        around this issue.\n",
      "  --skip_onnx_shape SKIP_ONNX_SHAPE\n",
      "                        Skip ONNX shape inference. Symbolic shape inference is\n",
      "                        most effective with transformer based models. Skipping\n",
      "                        all shape inferences may reduce the effectiveness of\n",
      "                        quantization, as a tensor with unknown shape can not\n",
      "                        be quantized.\n",
      "  --skip_symbolic_shape SKIP_SYMBOLIC_SHAPE\n",
      "                        Skip symbolic shape inference. Symbolic shape\n",
      "                        inference is most effective with transformer based\n",
      "                        models. Skipping all shape inferences may reduce the\n",
      "                        effectiveness of quantization, as a tensor with\n",
      "                        unknown shape can not be quantized.\n",
      "  --auto_merge          Automatically merge symbolic dims when confliction\n",
      "                        happens\n",
      "  --int_max INT_MAX     maximum value for integer to be treated as boundless\n",
      "                        for ops like slice\n",
      "  --guess_output_rank   guess output rank to be the same as input 0 for\n",
      "                        unknown ops\n",
      "  --verbose VERBOSE     Prints detailed logs of inference, 0: turn off, 1:\n",
      "                        warnings, 3: detailed\n",
      "  --save_as_external_data\n",
      "                        Saving an ONNX model to external data\n",
      "  --all_tensors_to_one_file\n",
      "                        Saving all the external data to one file\n",
      "  --external_data_location EXTERNAL_DATA_LOCATION\n",
      "                        The file location to save the external file\n",
      "  --external_data_size_threshold EXTERNAL_DATA_SIZE_THRESHOLD\n",
      "                        The size threshold for external data\n"
     ]
    }
   ],
   "source": [
    "!python3 -m onnxruntime.quantization.preprocess --input /home/razaare/unet.onnx --output /home/razaare/unet_infer/unet-infer.onnx --skip_symbolic_shape 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf523cb6-b53d-4581-a19a-dd46e2004a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    }
   ],
   "source": [
    "quantize_dynamic(\n",
    "    model_input=\"/home/razaare/unet.onnx\",\n",
    "    model_output=\"/home/razaare/unet_int8/unet_int8.onnx\",\n",
    "    per_channel=True,                          \n",
    "    weight_type=QuantType.QInt8,\n",
    "    use_external_data_format=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f719aef-1b9b-49a7-8799-98c01e122280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function quantize_dynamic in module onnxruntime.quantization.quantize:\n",
      "\n",
      "quantize_dynamic(model_input: 'str | Path | onnx.ModelProto', model_output: 'str | Path', op_types_to_quantize=None, per_channel=False, reduce_range=False, weight_type=<QuantType.QInt8: 0>, nodes_to_quantize=None, nodes_to_exclude=None, use_external_data_format=False, extra_options=None)\n",
      "    Given an onnx model, create a quantized onnx model and save it into a file\n",
      "    \n",
      "    Args:\n",
      "        model_input: file path of model or ModelProto to quantize\n",
      "        model_output: file path of quantized model\n",
      "        op_types_to_quantize:\n",
      "            specify the types of operators to quantize, like ['Conv'] to quantize Conv only.\n",
      "            It quantizes all supported operators by default.\n",
      "        per_channel: quantize weights per channel\n",
      "        reduce_range:\n",
      "            quantize weights with 7-bits. It may improve the accuracy for some models running on non-VNNI machine,\n",
      "            especially for per-channel mode\n",
      "        weight_type:\n",
      "            quantization data type of weight. Please refer to\n",
      "            https://onnxruntime.ai/docs/performance/quantization.html for more details on data type selection\n",
      "        nodes_to_quantize:\n",
      "            List of nodes names to quantize. When this list is not None only the nodes in this list\n",
      "            are quantized.\n",
      "            example:\n",
      "            [\n",
      "                'Conv__224',\n",
      "                'Conv__252'\n",
      "            ]\n",
      "        nodes_to_exclude:\n",
      "            List of nodes names to exclude. The nodes in this list will be excluded from quantization\n",
      "            when it is not None.\n",
      "        use_external_data_format: option used for large size (>2GB) model. Set to False by default.\n",
      "        extra_options:\n",
      "            key value pair dictionary for various options in different case. Current used:\n",
      "                extra.Sigmoid.nnapi = True/False  (Default is False)\n",
      "                ActivationSymmetric = True/False: symmetrize calibration data for activations (default is False).\n",
      "                WeightSymmetric = True/False: symmetrize calibration data for weights (default is True).\n",
      "                EnableSubgraph = True/False :\n",
      "                    Default is False. If enabled, subgraph will be quantized. Dynamic mode currently is supported. Will\n",
      "                    support more in the future.\n",
      "                ForceQuantizeNoInputCheck = True/False :\n",
      "                    By default, some latent operators like maxpool, transpose, do not quantize if their input is not\n",
      "                    quantized already. Setting to True to force such operator always quantize input and so generate\n",
      "                    quantized output. Also the True behavior could be disabled per node using the nodes_to_exclude.\n",
      "                MatMulConstBOnly = True/False:\n",
      "                    Default is True for dynamic mode. If enabled, only MatMul with const B will be quantized.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(quantize_dynamic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "942a8355-4c1a-4b0b-8fbc-e64ee0268d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/razaare/anaconda3/envs/fitcheck/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ[\"ORT_DISABLE_MEM_PATTERN\"]  = \"1\"\n",
    "os.environ[\"ORT_DISABLE_CPU_MEM_ARENA\"] = \"1\"\n",
    "os.environ[\"ORT_DISABLE_PREPACKING\"]    = \"1\"\n",
    "\n",
    "from diffusers import AutoencoderKL, DDIMScheduler\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from onnxruntime.quantization import (\n",
    "    QuantFormat, QuantType, CalibrationMethod,\n",
    "    quantize_static, CalibrationDataReader,\n",
    ")\n",
    "\n",
    "import onnxruntime as ort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31bb9da3-b85b-4ec7-8630-634cc6a77aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<onnxruntime.capi.onnxruntime_inference_collection.InferenceSession at 0x7fd8d82276d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess_options = ort.SessionOptions()\n",
    "sess_options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL       \n",
    "sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_BASIC\n",
    "sess_options.graph_optimization_level = (\n",
    "    ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED)\n",
    "sess_options.optimized_model_filepath = \"unet_ort_opt.onnx\"\n",
    "ort.InferenceSession(\"/home/razaare/unet.onnx\", sess_options, providers=[\"CPUExecutionProvider\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30d3fd73-a032-497b-adbf-11b3e27f4696",
   "metadata": {},
   "outputs": [],
   "source": [
    "so = ort.SessionOptions()\n",
    "so.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL       \n",
    "so.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_BASIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "374ebdb9-d905-4db9-b27c-f6a6854064fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_ckpt = \"booksforcharlie/stable-diffusion-inpainting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "595f4d64-54f7-4e36-9512-dd2be5b937b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_scheduler = DDIMScheduler.from_pretrained(base_ckpt, subfolder=\"scheduler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02be5406-cadc-45f1-8113-5e39799485a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"/home/razaare/calibration_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52166c35-9d68-4716-b783-6fca32817ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_calib_set(out_dir: Path, n_samples: int, sched: DDIMScheduler, seed: int | None = None):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    h, w = 256, 96\n",
    "    for i in range(n_samples):\n",
    "        t = rng.integers(0, sched.num_train_timesteps, dtype=np.int64)\n",
    "        t_tensor = torch.tensor([t], dtype=torch.long)\n",
    "    \n",
    "        eps = torch.randn(1, 4, h, w, dtype=torch.float32)\n",
    "        x_t = sched.add_noise(torch.zeros_like(eps), eps, t_tensor)\n",
    "    \n",
    "        mask_ratio = rng.uniform(0.1, 0.9)\n",
    "        mask = (torch.rand(1, 1, h, w) < mask_ratio).float()\n",
    "        x0 = torch.randn_like(x_t)\n",
    "        masked = x0 * (1.0 - mask)\n",
    "    \n",
    "        latent = torch.cat([x_t, masked, mask], dim=1)  # 1×9×256×96\n",
    "    \n",
    "        np.savez_compressed(\n",
    "            out_dir / f\"s{i:05}.npz\",\n",
    "            latent=latent.numpy(),\n",
    "            t=np.array([t], dtype=np.int64),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8205555a-6dd3-4caa-a63c-3ddebdac4e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_calib_set(path, 512, noise_scheduler, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bae7d237-6463-4cbe-957d-e5ef1911ba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InpaintReader(CalibrationDataReader):\n",
    "    def __init__(self, root: Path):\n",
    "        self.files = list(root.glob(\"*.npz\"))\n",
    "        self._it = iter(self.files)\n",
    "\n",
    "    def get_next(self):\n",
    "        try:\n",
    "            f = next(self._it)\n",
    "        except StopIteration:\n",
    "            return None\n",
    "        d = np.load(f)\n",
    "        return {\"sample\": d[\"latent\"], \"timestep\": d[\"t\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aaa8b2b6-8c7f-4c77-bcbf-3ca6d3dbebca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function quantize_static in module onnxruntime.quantization.quantize:\n",
      "\n",
      "quantize_static(model_input: 'str | Path | onnx.ModelProto', model_output: 'str | Path', calibration_data_reader: 'CalibrationDataReader', quant_format=<QuantFormat.QDQ: 1>, op_types_to_quantize=None, per_channel=False, reduce_range=False, activation_type=<QuantType.QInt8: 0>, weight_type=<QuantType.QInt8: 0>, nodes_to_quantize=None, nodes_to_exclude=None, use_external_data_format=False, calibrate_method=<CalibrationMethod.MinMax: 0>, calibration_providers=None, extra_options=None)\n",
      "    Given an onnx model and calibration data reader, create a quantized onnx model and save it into a file\n",
      "    It is recommended to use QuantFormat.QDQ format from 1.11 with activation_type = QuantType.QInt8 and weight_type\n",
      "    = QuantType.QInt8. If model is targeted to GPU/TRT, symmetric activation and weight are required. If model is\n",
      "    targeted to CPU, asymmetric activation and symmetric weight are recommended for balance of performance and\n",
      "    accuracy.\n",
      "    \n",
      "    Args:\n",
      "    \n",
      "        model_input: file path of model or ModelProto to quantize\n",
      "        model_output: file path of quantized model\n",
      "        calibration_data_reader: a calibration data reader. It\n",
      "            enumerates calibration data and generates inputs for the\n",
      "            original model.\n",
      "        quant_format: QuantFormat{QOperator, QDQ}.\n",
      "            QOperator format quantizes the model with quantized operators directly.\n",
      "            QDQ format quantize the model by inserting QuantizeLinear/DeQuantizeLinear on the tensor.\n",
      "        activation_type:\n",
      "            quantization data type of activation. Please refer to\n",
      "            https://onnxruntime.ai/docs/performance/quantization.html for more details on data type selection\n",
      "        calibrate_method:\n",
      "            Current calibration methods supported are MinMax and Entropy.\n",
      "                Please use CalibrationMethod.MinMax or CalibrationMethod.Entropy as options.\n",
      "        op_types_to_quantize:\n",
      "                specify the types of operators to quantize, like ['Conv'] to quantize Conv only.\n",
      "                It quantizes all supported operators by default.\n",
      "        per_channel: quantize weights per channel\n",
      "        reduce_range:\n",
      "            quantize weights with 7-bits. It may improve the accuracy for some models running on non-VNNI machine,\n",
      "            especially for per-channel mode\n",
      "        weight_type:\n",
      "            quantization data type of weight. Please refer to\n",
      "            https://onnxruntime.ai/docs/performance/quantization.html for more details on data type selection\n",
      "        nodes_to_quantize:\n",
      "            List of nodes names to quantize. When this list is not None only the nodes in this list\n",
      "            are quantized.\n",
      "            example:\n",
      "            [\n",
      "                'Conv__224',\n",
      "                'Conv__252'\n",
      "            ]\n",
      "        nodes_to_exclude:\n",
      "            List of nodes names to exclude. The nodes in this list will be excluded from quantization\n",
      "            when it is not None.\n",
      "        use_external_data_format: option used for large size (>2GB) model. Set to False by default.\n",
      "        calibration_providers: Execution providers to run the session during calibration. Default is None which uses\n",
      "            [ \"CPUExecutionProvider\" ]\n",
      "        extra_options:\n",
      "            key value pair dictionary for various options in different case. Current used:\n",
      "                extra.Sigmoid.nnapi = True/False  (Default is False)\n",
      "                ActivationSymmetric = True/False: symmetrize calibration data for activations (default is False).\n",
      "                WeightSymmetric = True/False: symmetrize calibration data for weights (default is True).\n",
      "                EnableSubgraph = True/False : Default is False. If enabled, subgraph will be quantized.\n",
      "                                              Dyanmic mode currently is supported. Will support more in the future.\n",
      "                ForceQuantizeNoInputCheck = True/False :\n",
      "                    By default, some latent operators like maxpool, transpose, do not quantize if their input is not\n",
      "                    quantized already. Setting to True to force such operator always quantize input and so generate\n",
      "                    quantized output. Also, the True behavior could be disabled per node using the nodes_to_exclude.\n",
      "                MatMulConstBOnly = True/False:\n",
      "                    Default is False for static mode. If enabled, only MatMul with const B will be quantized.\n",
      "                AddQDQPairToWeight = True/False :\n",
      "                    Default is False which quantizes floating-point weight and feeds it to solely inserted\n",
      "                    DeQuantizeLinear node. If True, it remains floating-point weight and inserts both\n",
      "                    QuantizeLinear/DeQuantizeLinear nodes to weight.\n",
      "                OpTypesToExcludeOutputQuantization = list of op type :\n",
      "                    Default is []. If any op type is specified, it won't quantize the output of ops with this\n",
      "                    specific op types.\n",
      "                DedicatedQDQPair = True/False :\n",
      "                    Default is False. When inserting QDQ pair, multiple nodes can share a single QDQ pair as their\n",
      "                    inputs. If True, it will create identical and dedicated QDQ pair for each node.\n",
      "                QDQOpTypePerChannelSupportToAxis = dictionary :\n",
      "                    Default is {}. Set channel axis for specific op type, for example: {'MatMul': 1}, and it's\n",
      "                    effective only when per channel quantization is supported and per_channel is True. If specific\n",
      "                    op type supports per channel quantization but not explicitly specified with channel axis,\n",
      "                    default channel axis will be used.\n",
      "                CalibTensorRangeSymmetric = True/False :\n",
      "                    Default is False. If enabled, the final range of tensor during calibration will be explicitly\n",
      "                    set to symmetric to central point \"0\".\n",
      "                CalibStridedMinMax = Optional[int] :\n",
      "                    Default is None. If set to an integer, during calculation of the min-max, only stride amount of\n",
      "                    data will be used and then all results will be merged in the end.\n",
      "                CalibMovingAverage = True/False :\n",
      "                    Default is False. If enabled, the moving average of the minimum and maximum values will be\n",
      "                    computed when the calibration method selected is MinMax.\n",
      "                CalibMovingAverageConstant = float :\n",
      "                    Default is 0.01. Constant smoothing factor to use when computing the moving average of the\n",
      "                    minimum and maximum values. Effective only when the calibration method selected is MinMax and\n",
      "                    when CalibMovingAverage is set to True.\n",
      "                CalibMaxIntermediateOutputs = Optional[int] :\n",
      "                    Default is None. If set to an integer, during calculation of the min-max range of the tensors\n",
      "                    it will load at max value number of outputs before computing and merging the range. This will\n",
      "                    produce the same result as all computing with None, but is more memory efficient.\n",
      "                SmoothQuant = True/False :\n",
      "                    Default is False. If enabled, SmoothQuant algorithm will be applied before quantization to do\n",
      "                    fake input channel quantization.\n",
      "                SmoothQuantAlpha = float :\n",
      "                    Default is 0.5. It only works if SmoothQuant is True. It controls the difficulty of weight\n",
      "                    and activation quantization. A larger alpha value could be used on models with more significant\n",
      "                    activation outliers to migrate more quantization difficulty to weights.\n",
      "                SmoothQuantFolding = True/False :\n",
      "                    Default is True. It only works if SmoothQuant is True. If enabled, inserted Mul ops during\n",
      "                    SmoothQuant will be folded into the previous op if the previous op is foldable.\n",
      "                UseQDQContribOps = True/False :\n",
      "                    Default is False. If enabled, the inserted QuantizeLinear and DequantizeLinear ops will have the\n",
      "                    `com.microsoft` domain, which forces use of ONNX Runtime's QuantizeLinear and DequantizeLinear\n",
      "                    contrib op implementations. The contrib op implementations may support features not standardized\n",
      "                    into the ONNX specification (e.g., 16-bit quantization types).\n",
      "                MinimumRealRange = float|None :\n",
      "                    Default is None. If set to a floating-point value, the calculation of the quantization parameters\n",
      "                    (i.e., scale and zero point) will enforce a minimum range between rmin and rmax. If (rmax - rmin)\n",
      "                    is less than the specified minimum range, rmax will be set to rmin + MinimumRealRange. This is\n",
      "                    necessary for EPs like QNN that require a minimum floating-point range when determining\n",
      "                    quantization parameters.\n",
      "                TensorQuantOverrides = dictionary :\n",
      "                    Default is {}. Set tensor quantization overrides. The key is a tensor name and the value is a\n",
      "                    list of dictionaries. For per-tensor quantization, the list contains a single dictionary. For\n",
      "                    per-channel quantization, the list contains a dictionary for each channel in the tensor.\n",
      "                    Each dictionary contains optional overrides with the following keys and values.\n",
      "                        'quant_type' = QuantType : The tensor's quantization data type.\n",
      "                        'scale' =  Float         : The scale value to use. Must also specify `zero_point` if set.\n",
      "                        'zero_point' = Int       : The zero-point value to use. Must also specify `scale` is set.\n",
      "                        'symmetric' = Bool       : If the tensor should use symmetric quantization. Invalid if also\n",
      "                                                   set `scale` or `zero_point`.\n",
      "                        'reduce_range' = Bool    : If the quantization range should be reduced. Invalid if also\n",
      "                                                   set `scale` or `zero_point`.\n",
      "                        'rmax' = Float           : Override the maximum real tensor value in calibration data.\n",
      "                                                   Invalid if also set `scale` or `zero_point`.\n",
      "                        'rmin' = Float           : Override the minimum real tensor value in calibration data.\n",
      "                                                   Invalid if also set `scale` or `zero_point`.\n",
      "                QDQKeepRemovableActivations = True/False:\n",
      "                    Default is False. If true, \"removable\" activations (e.g., Clip or Relu) will not be removed, and\n",
      "                    will be explicitly represented in the QDQ model. If false, these activations are automatically\n",
      "                    removed if activations are asymmetrically quantized. Keeping these activations is necessary if\n",
      "                    optimizations or EP transformations will later remove QuantizeLinear/DequantizeLinear\n",
      "                    operators from the model.\n",
      "                QDQDisableWeightAdjustForInt32Bias = True/False:\n",
      "                    Default is False. If true, QDQ quantizer will not adjust the weight's scale when the bias\n",
      "                    has a scale (input_scale * weight_scale) that is too small.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(quantize_static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6bcc4f-7107-4588-ac75-6e406a563b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    }
   ],
   "source": [
    "reader = InpaintReader(Path(\"/home/razaare/calibration_dataset\"))\n",
    "quantize_static(\n",
    "    model_input=\"/home/razaare/unet_ort_opt.onnx\",\n",
    "    model_output=\"home/razaare/unet_int8_static/unet_int8_static.onnx\",\n",
    "    calibration_data_reader=reader,\n",
    "    quant_format=QuantFormat.QDQ,\n",
    "    weight_type=QuantType.QInt8,\n",
    "    activation_type=QuantType.QInt8,\n",
    "    calibrate_method=CalibrationMethod.MinMax,\n",
    "    use_external_data_format=True,\n",
    "    per_channel=False,\n",
    "    extra_options={\n",
    "        \"CalibMaxIntermediateOutputs\": 2,\n",
    "        \"CalibMovingAverage\": True,         \n",
    "        \"CalibMovingAverageConstant\": 0.01,\n",
    "        \"ActivationSymmetric\": True,\n",
    "        \"WeightSymmetric\": True,\n",
    "        \"DisableShapeInference\": True,\n",
    "    },\n",
    "    calibration_providers=[\"CPUExecutionProvider\"],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceb8984-88e6-4d17-8290-e58822bdbe46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
